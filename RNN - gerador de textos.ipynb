{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktS82YxUE8So"
      },
      "source": [
        "# RNN with tensorflow2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81fMvmIME8Sp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRCnB2oYE8Sq"
      },
      "source": [
        "## Be sure to used Tensorflow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEqBlZWUE8Sq"
      },
      "outputs": [],
      "source": [
        "assert hasattr(tf, \"function\") # Be sure to use tensorflow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrizc-zE8Sq"
      },
      "source": [
        "## Open and process dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G8Z_3TmE8Sq",
        "outputId": "952ffbe8-2003-491f-8a7f-7279e4296970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127286\n",
            "Parce que, jargonnant vêpres, jeûne et vigile,\n",
            "Exploitant Dieu qui rêve au fond du firmament,\n",
            "Vous avez, au milieu du divin évangile,\n",
            "Ouvert boutique effrontément ;\n",
            "\n",
            "Parce que vous feriez prendre à Jésus la verge,\n",
            "Cyniques brocanteurs sortis on ne sait d'où ;\n",
            "Parce que vous allez vendant la sainte vierge\n",
            "Dix sous avec miracle, et sans miracle un sou ;\n",
            "\n",
            "Parce que vous contez d'effroyables sornettes\n",
            "Qui font des temples saints trembler les vieux piliers ;\n",
            "Parce que votre style éblouit les lunettes\n",
            "Des duègnes et des marguilliers ;\n",
            "\n",
            "Parce que la soutane est sous vos redingotes,\n",
            "Parce que vous sentez la crasse et non l'œillet,\n",
            "Parce que vous bâclez un journal de bigotes\n",
            "Pensé par Escobar, écrit par Patouillet ;\n",
            "\n",
            "Parce qu'en balayant leurs portes, les concierges\n",
            "Poussent dans le ruisseau ce pamphlet méprisé ;\n",
            "Parce que vous mêlez à la cire des cierges\n",
            "Votre affreux suif vert-de-grisé ;\n",
            "\n",
            "Parce qu'à vous tout seuls vous faites une espèce\n",
            "Parce qu'enfin, blanchis dehors et noirs dedans,\n",
            "Criant\n"
          ]
        }
      ],
      "source": [
        "# You can used your own dataset with english text\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/thibo73800/tensorflow2.0-examples/master/rnn_dataset/victorhugo.txt\"\n",
        "\n",
        "with urllib.request.urlopen(url) as f:\n",
        "    text = f.read().decode()\n",
        "\n",
        "print(len(text))\n",
        "\n",
        "print(text[:1000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSUp8BU9E8Sr"
      },
      "source": [
        "## Remove character and create vocab\n",
        "![](./images/rnn_vocab.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB03rR9UFl0x",
        "outputId": "3e35794e-5c43-4b23-9c3e-860c6c3d6838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvfiZKxbE8Sr",
        "outputId": "113e0b3c-9006-4300-abed-d2ca3ffc895d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 {'c', 'i', 'b', 'j', 'u', 'n', ' ', 'y', 'h', 'k', 'r', '\\n', 'o', 'e', 'x', 'g', 'f', 'm', 'w', 'q', 'z', \"'\", 's', 'p', 't', ':', ',', 'v', 'a', '\"', 'l', '.', 'd'}\n",
            "parce que, jargonnant vepres, jeune et vigile,\n",
            "exploitant dieu qui reve au fond du firmament,\n",
            "vous a\n"
          ]
        }
      ],
      "source": [
        "import unidecode\n",
        "\n",
        "text = unidecode.unidecode(text)\n",
        "text = text.lower()\n",
        "\n",
        "text = text.replace(\"2\", \"\")\n",
        "text = text.replace(\"1\", \"\")\n",
        "text = text.replace(\"8\", \"\")\n",
        "text = text.replace(\"5\", \"\")\n",
        "text = text.replace(\">\", \"\")\n",
        "text = text.replace(\"<\", \"\")\n",
        "text = text.replace(\"!\", \"\")\n",
        "text = text.replace(\"?\", \"\")\n",
        "text = text.replace(\"-\", \"\")\n",
        "text = text.replace(\"$\", \"\")\n",
        "text = text.replace(\";\", \"\")\n",
        "\n",
        "text = text.strip()\n",
        "\n",
        "vocab = set(text)\n",
        "print(len(vocab), vocab)\n",
        "\n",
        "print(text[:100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6Dy1SBVE8Sr"
      },
      "source": [
        "## Map each letter to int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eD4e4PjE8Sr",
        "outputId": "12dbae33-7a28-4797-b608-f4ff1a4ad7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_to_int {'c': 0, 'i': 1, 'b': 2, 'j': 3, 'u': 4, 'n': 5, ' ': 6, 'y': 7, 'h': 8, 'k': 9, 'r': 10, '\\n': 11, 'o': 12, 'e': 13, 'x': 14, 'g': 15, 'f': 16, 'm': 17, 'w': 18, 'q': 19, 'z': 20, \"'\": 21, 's': 22, 'p': 23, 't': 24, ':': 25, ',': 26, 'v': 27, 'a': 28, '\"': 29, 'l': 30, '.': 31, 'd': 32}\n",
            "\n",
            "int_to_vocab {0: 'c', 1: 'i', 2: 'b', 3: 'j', 4: 'u', 5: 'n', 6: ' ', 7: 'y', 8: 'h', 9: 'k', 10: 'r', 11: '\\n', 12: 'o', 13: 'e', 14: 'x', 15: 'g', 16: 'f', 17: 'm', 18: 'w', 19: 'q', 20: 'z', 21: \"'\", 22: 's', 23: 'p', 24: 't', 25: ':', 26: ',', 27: 'v', 28: 'a', 29: '\"', 30: 'l', 31: '.', 32: 'd'}\n",
            "\n",
            "int for e: 13\n",
            "letter for 13: e\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "vocab_to_int = {l:i for i,l in enumerate(vocab)}\n",
        "int_to_vocab = {i:l for i,l in enumerate(vocab)}\n",
        "\n",
        "print(\"vocab_to_int\", vocab_to_int)\n",
        "print()\n",
        "print(\"int_to_vocab\", int_to_vocab)\n",
        "\n",
        "print(\"\\nint for e:\", vocab_to_int[\"e\"])\n",
        "int_for_e = vocab_to_int[\"e\"]\n",
        "print(\"letter for %s: %s\" % (vocab_to_int[\"e\"], int_to_vocab[int_for_e]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AMqXns3E8Ss",
        "outputId": "04f87a88-f519-49a0-cfd9-b83b0ff0f149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23, 28, 10, 0, 13, 6, 19, 4, 13, 26, 6, 3, 28, 10, 15, 12, 5, 5, 28, 5, 24, 6, 27, 13, 23, 10, 13, 22, 26, 6, 3, 13, 4, 5, 13, 6, 13, 24, 6, 27, 1, 15, 1, 30, 13, 26, 11, 13, 14, 23, 30, 12, 1, 24, 28, 5, 24, 6, 32, 1, 13, 4, 6, 19, 4, 1, 6, 10, 13, 27, 13, 6, 28, 4, 6, 16, 12, 5, 32, 6, 32, 4, 6, 16, 1, 10, 17, 28, 17, 13, 5, 24, 26, 11, 27, 12, 4, 22, 6, 28]\n"
          ]
        }
      ],
      "source": [
        "encoded = [vocab_to_int[l] for l in text]\n",
        "encoded_sentence = encoded[:100]\n",
        "\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9nQ0zkqE8Ss",
        "outputId": "4fbdf027-6a1e-4aed-800c-81d4b5fd6c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p', 'a', 'r', 'c', 'e', ' ', 'q', 'u', 'e', ',', ' ', 'j', 'a', 'r', 'g', 'o', 'n', 'n', 'a', 'n', 't', ' ', 'v', 'e', 'p', 'r', 'e', 's', ',', ' ', 'j', 'e', 'u', 'n', 'e', ' ', 'e', 't', ' ', 'v', 'i', 'g', 'i', 'l', 'e', ',', '\\n', 'e', 'x', 'p', 'l', 'o', 'i', 't', 'a', 'n', 't', ' ', 'd', 'i', 'e', 'u', ' ', 'q', 'u', 'i', ' ', 'r', 'e', 'v', 'e', ' ', 'a', 'u', ' ', 'f', 'o', 'n', 'd', ' ', 'd', 'u', ' ', 'f', 'i', 'r', 'm', 'a', 'm', 'e', 'n', 't', ',', '\\n', 'v', 'o', 'u', 's', ' ', 'a']\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = [int_to_vocab[i] for i in encoded_sentence]\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYTbxCK6E8Ss",
        "outputId": "a185934f-32e9-454c-8029-113c81a992d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parce que, jargonnant vepres, jeune et vigile,\n",
            "exploitant dieu qui reve au fond du firmament,\n",
            "vous a\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = \"\".join(decoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bndwy59UE8Ss"
      },
      "source": [
        "## Sample of one batch\n",
        "<img src=\"./images/rnn_letter.png\" width=\"400px\" ></img>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrrx1T0EE8Ss",
        "outputId": "f38bd38a-a455-45d2-dc56-8f36366e0ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs [23, 28, 10, 0, 13, 6, 19, 4, 13, 26]\n",
            "Targets [28, 10, 0, 13, 6, 19, 4, 13, 26, 6]\n"
          ]
        }
      ],
      "source": [
        "inputs, targets = encoded, encoded[1:]\n",
        "\n",
        "print(\"Inputs\", inputs[:10])\n",
        "print(\"Targets\", targets[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0aEdsj_E8St"
      },
      "source": [
        "## Method used to generate batch in sequence order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3JNrgYTE8St",
        "outputId": "1d59382b-1353-4165-f434-ee46b832ce1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23. 28. 10.  0. 13.] [28. 10.  0. 13.  6.] (32, 5) (32, 5)\n",
            "[18. 28. 10. 18. 13.] [28. 10.  0. 13.  6.]\n"
          ]
        }
      ],
      "source": [
        "def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
        "    # Size of each chunk\n",
        "    chuck_size = (len(inputs) -1)  // batch_size\n",
        "    # Numbef of sequence per chunk\n",
        "    sequences_per_chunk = chuck_size // seq_len\n",
        "\n",
        "    for s in range(0, sequences_per_chunk):\n",
        "        batch_inputs = np.zeros((batch_size, seq_len))\n",
        "        batch_targets = np.zeros((batch_size, seq_len))\n",
        "        for b in range(0, batch_size):\n",
        "            fr = (b*chuck_size)+(s*seq_len)\n",
        "            to = fr+seq_len\n",
        "            batch_inputs[b] = inputs[fr:to]\n",
        "            batch_targets[b] = inputs[fr+1:to+1]\n",
        "            \n",
        "            if noise > 0:\n",
        "                noise_indices = np.random.choice(seq_len, noise)\n",
        "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
        "            \n",
        "        yield batch_inputs, batch_targets\n",
        "\n",
        "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=0):\n",
        "    print(batch_inputs[0], batch_targets[0],batch_inputs.shape, batch_targets.shape)\n",
        "    break\n",
        "\n",
        "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=3):\n",
        "    print(batch_inputs[0], batch_targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0awDsZdE8St"
      },
      "source": [
        "## Create your own layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qgzAWVhE8St"
      },
      "outputs": [],
      "source": [
        "class OneHot(tf.keras.layers.Layer):\n",
        "    def __init__(self, depth, **kwargs):\n",
        "        super(OneHot, self).__init__(**kwargs)\n",
        "        self.depth = depth\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3I-K4ieE8Su"
      },
      "source": [
        "Test if the layer works well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG-icNklE8Su",
        "outputId": "79e53750-0f96-430f-cd83-8fb1f6b089b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 50)\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "(64, 50, 33)\n",
            "Input letter is: 23.0\n",
            "One hot representation of the letter [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "class RnnModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super(RnnModel, self).__init__()\n",
        "        # Convolutions\n",
        "        self.one_hot = OneHot(len(vocab))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = self.one_hot(inputs)\n",
        "        return output\n",
        "\n",
        "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 64))\n",
        "\n",
        "print(batch_inputs.shape)\n",
        "\n",
        "model = RnnModel(len(vocab))\n",
        "output = model.predict(batch_inputs)\n",
        "\n",
        "print(output.shape)\n",
        "\n",
        "#print(output)\n",
        "\n",
        "print(\"Input letter is:\", batch_inputs[0][0])\n",
        "print(\"One hot representation of the letter\", output[0][0])\n",
        "\n",
        "#assert(output[int(batch_inputs[0][0])]==1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scDdriNUE8Su"
      },
      "source": [
        "# Set up the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YCNcAE_E8Su"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq8pu0ZrE8Su",
        "outputId": "6920e2ef-60bd-4eb3-e672-d00da8098f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(32, None, 128), dtype=tf.float32, name=None), name='lstm_28/PartitionedCall:1', description=\"created by layer 'lstm_28'\")\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "### Creat the layers\n",
        "\n",
        "# Set the input of the model\n",
        "tf_inputs = tf.keras.Input(shape=(None,), batch_size=32)\n",
        "# Convert each value of the  input into a one encoding vector\n",
        "one_hot = OneHot(len(vocab))(tf_inputs)\n",
        "# Stack LSTM cells\n",
        "rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot)\n",
        "rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
        "print(rnn_layer2)\n",
        "\n",
        "# Create the outputs of the model\n",
        "hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(rnn_layer2)\n",
        "outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
        "\n",
        "### Setup the model\n",
        "model = tf.keras.Model(inputs=tf_inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvvVe515E8Su"
      },
      "source": [
        "## Check if we can reset the RNN cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "AC7WXgG5E8Su",
        "outputId": "4e699fa6-5d53-4bad-99f6-18fda69b6902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 50) (64, 50)\n",
            "2/2 [==============================] - 1s 67ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Reset the states of the RNN states\\nmodel.reset_states()\\n\\n# Make an other prediction to check the difference\\noutputs = model.predict(batch_inputs)\\nsecond_prediction = outputs[0][0]\\n\\n# Check if both prediction are equal\\nassert(set(first_prediction)==set(second_prediction))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "# Star by resetting the cells of the RNN\n",
        "model.reset_states()\n",
        "\n",
        "# Get one batch\n",
        "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 64))\n",
        "\n",
        "print(batch_inputs.shape, batch_targets.shape)\n",
        "\n",
        "# Make a first prediction\n",
        "outputs = model.predict(batch_inputs)\n",
        "first_prediction = outputs[0][0]\n",
        "\n",
        "'''\n",
        "# Reset the states of the RNN states\n",
        "model.reset_states()\n",
        "\n",
        "# Make an other prediction to check the difference\n",
        "outputs = model.predict(batch_inputs)\n",
        "second_prediction = outputs[0][0]\n",
        "\n",
        "# Check if both prediction are equal\n",
        "assert(set(first_prediction)==set(second_prediction))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVx1og4eE8Sv"
      },
      "source": [
        "## Set the loss and objectives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osZdNE_AE8Sv"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp6udBX4E8Sv"
      },
      "source": [
        "## Set some metrics to track the progress of the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgbvmF5dE8Sv"
      },
      "outputs": [],
      "source": [
        "# Loss\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "# Accuracy\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu3bFmA2E8Sv"
      },
      "source": [
        "## Set the train method and the predict method in graph mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4aY5tnrE8Sv"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Make a prediction on all the batch\n",
        "        predictions = model(inputs)\n",
        "        # Get the error/loss on these predictions\n",
        "        loss = loss_object(targets, predictions)\n",
        "    # Compute the gradient which respect to the loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    # Change the weights of the model\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
        "    train_loss(loss)\n",
        "    train_accuracy(targets, predictions)\n",
        "\n",
        "@tf.function\n",
        "def predict(inputs):\n",
        "    # Make a prediction on all the batch\n",
        "    predictions = model(inputs)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRQ2hUOOE8Sv"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "metadata": {
        "id": "9J17RIo2TYyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wIrAr0gE8Sv"
      },
      "outputs": [],
      "source": [
        "model.reset_states()\n",
        "\n",
        "for epoch in range(4000):\n",
        "    for batch_inputs, batch_targets in gen_batch(inputs, targets, 100, 32, noise=13):\n",
        "        train_step(batch_inputs, batch_targets)\n",
        "    template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
        "    print(template.format(epoch, train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
        "    model.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cgZWwIPJd7wB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5KrvNJIE8Sv"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmZF1_xjE8Sw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "model.save(\"model_rnn.h5\")\n",
        "\n",
        "with open(\"model_rnn_vocab_to_int\", \"w\") as f:\n",
        "    f.write(json.dumps(vocab_to_int))\n",
        "with open(\"model_rnn_int_to_vocab\", \"w\") as f:\n",
        "    f.write(json.dumps(int_to_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7D1pp2E8Sw"
      },
      "source": [
        "# Generate some text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h1SOk15E8Sw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "model.reset_states()\n",
        "\n",
        "size_poetries = 300\n",
        "\n",
        "poetries = np.zeros((64, size_poetries, 1))\n",
        "sequences = np.zeros((64, 100))\n",
        "for b in range(64):\n",
        "    rd = np.random.randint(0, len(inputs) - 100)\n",
        "    sequences[b] = inputs[rd:rd+100]\n",
        "\n",
        "for i in range(size_poetries+1):\n",
        "    if i > 0:\n",
        "        poetries[:,i-1,:] = sequences\n",
        "    softmax = predict(sequences)\n",
        "    # Set the next sequences\n",
        "    sequences = np.zeros((64, 1))\n",
        "    for b in range(64):\n",
        "        argsort = np.argsort(softmax[b][0])\n",
        "        argsort = argsort[::-1]\n",
        "        # Select one of the strongest 4 proposals\n",
        "        sequences[b] = argsort[0]\n",
        "\n",
        "for b in range(64):\n",
        "    sentence = \"\".join([int_to_vocab[i[0]] for i in poetries[b]])\n",
        "    print(sentence)\n",
        "    print(\"\\n=====================\\n\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4cuNKekE8Sw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"model_rnn_vocab_to_int\", \"r\") as f:\n",
        "    vocab_to_int = json.loads(f.read())\n",
        "with open(\"model_rnn_int_to_vocab\", \"r\") as f:\n",
        "    int_to_vocab = json.loads(f.read())\n",
        "    int_to_vocab = {int(key):int_to_vocab[key] for key in int_to_vocab}\n",
        "\n",
        "model.load_weights(\"model_rnn.h5\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}